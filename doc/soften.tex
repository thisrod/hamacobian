\input a4
\input amssym
\input xpmath

% xetex soften && open soften.pdf

\def\z{z}
\def\D{D}

\centerline{\bf Algorithms to soften coherent state Dirac-Frenkel dynamics}
\vskip 3ex
\centerline{by Rodney Polkinghorne, Swinburne University of Technology, Melbourne}
\vskip 5ex

Peter and I are solving Schrödinger's equation variationally, with the ansatz that the wavefunction is a linear combination of coherent states in a fixed set of modes for single particles.  The Dirac-Frenkel method produces an ill-conditioned least squares problem, which we attempted to solve by some standard methods, Tychonoff regularisation, and by adding an imaginary part to the badly conditioned matrix.  When we calculated derivatives of the coherent amplitudes that way, integrating them numerically required tens of thousands of timesteps per revival, which was impractical.  I believe this happens because the least squares differential equations are stiff.  There follow some methods that might produce less stiff dynamics.

\beginsection{Mappings from ${\bf C}^n$ into Hilbert space}

To solve the Schr\"odinger equation variationally, we map some parameters $z$ in ${\bf C}^r$ to a ket $|ψ(z)〉$.  The function $|ψ〉$ is nonlinear---otherwise, we could restrict the dynamics to its range, an $r$ dimensional subspace of kets, and avoid the complications below---but our computers approximate it with the total derivative $|\D ψ〉$, a linear mapping that could be expanded as a Jacobian matrix of complex numbers if the dimension of the ket space were finite, but, to avoid infinitely long columns, must actually be left as a row vector of kets.

In a finite dimensional vector space, everything can be reduced to complex numbers by setting axes and taking components.  Because computers work with a finite set of kets at a time, all the spaces they represent are finite dimensional.  However, a good set of axes for the space might not be readily available.  For mathematical purposes, the partial derivatives of the representation function $|ψ〉$ are linearly independent: no linear combination of them is exactly zero.  However, for numerical purposes, there is no such thing as exact equality, only approximation.  Linear combinations of the partial derivatives approximate zero very well, so, for numerical purposes, they are linearly dependent.  We will need to work in the infinite dimensional ket space, and construct axes as we go.

For this end, and for other purposes, we will use matrices that combine complex numbers with bras, kets and operators.  The idea is that a ket has been split into components along a finite number of axes, and the remainder in the space othogonal to those axes has been left as a ket.  Kets are naturally column vectors: operators multiply them on the left.  So a ket might occur as the last element of a column vector: $$|x〉=(x₁\ x₂\ …\ x_m\ |x_\perp〉)^{\rm T}.$$  We will write all matrix sizes as dimensions over complex numbers, so this column has size $(m+∞)×1$.  Here, $m+∞$ means $m$ complex numbers, and one ket.

We can assemble these columns into a matrices.  The simplest case is a row vector of kets, $$|A〉=(|a₁〉\ |a₂〉\ …\ |a_n〉).$$  The kets are infinite columns, so this has size $∞×n$.  It can form products with matrices of complex numbers.  Its product with an $n× r$ complex matrix $B$ on the right is a $∞×n$ row of kets $|A〉 B$, whose $i$th element is the linear combination $|A〉 b_i=|a₁〉b_{1i}+⋯+|a_n〉b_{ni}$.  More generally, we could have a $∞×(n+∞)$ matrix $$|U〉=(|e₁〉\ |e₂〉\ …\ |e_n〉\ I).$$  Here, the identity operator $I$ is considered an $∞×∞$ matrix.  This matrix can muliply an $(n+∞)×1$ column vector $|c〉=(c₁\ c₂\ …\ c_n\ |c_\perp〉)^{\rm T}$, to give the ket $(c₁|e₁〉+…+c_n|e_n〉+|c_\perp〉)$.  In this way, we can split a ket into components over a finite number of axes, and leave the rest as a ket.  This will allow us to do Householder reflections in Hilbert space, and solve least squares problems in a not entirely unstable manner.  When this is done on a computer, the components will be stored as vectors of complex numbers, and the kets will be stored more abstractly.

The rule for well-formedness is simple: when counted as dimensions over the complex numbers, all rows of a matrix must have the same length, as must all columns.  There is a slight subtlety, because a number can be either a $1×1$ matrix, or an $∞×∞$ operator.  But this will generally be clear from context.  As a matter of notation, matrices with infinitely many rows but finitely many columns will be denoted as bras, their transposes as kets, and matrices with infinitely many rows and columns will be written as operators.

As kets are infinite column vectors, bras are infinite row vectors.  The column $|A〉$ can also form products with bras.  Its product with a column $〈 B|=(|b₁〉,…,|b_r〉)*$ on the right is a linear operator on ket space $|A〉〈 B|=|a₁〉〈 b₁|+⋯+|a_r〉〈 b_r|$.  Finally, its product  by a column vector $〈 C|=(|c₁〉,…,|c_n〉)*$ on the left is an $n× r$ matrix of complex numbers $〈C|A〉$, whose $ij$th element is $〈 c_i|a_j〉$.

The most general form of matrix is an operator partially expanded along both rows and columns, as follows: $$A=\pmatrix{a_{11} & & a_{1n} & 〈a₁| \cr & \ddots & \cr a_{m1} &  & a_{mn} & 〈a_{m}| \cr |a₁〉& &|a_n〉& A }.$$  This maps kets that have been partially expanded as $(n+∞)×1$ columns into kets expanded as $(m+∞)×1$ columns.  In case $m=n$, there is an identity matrix, $$I=\pmatrix{1 & & 0 & 〈∅| \cr & \ddots & \cr 0 &  & 1 & 〈∅| \cr |∅〉& &|∅〉& {\bf 1} }.$$  Here, $|∅〉$ is the zero ket, and $\bf 1$ the identity operator.

These matrices have hermitian conjugates in the obvious way, by transposing the matrix, and taking complex or hermitian conjugates of the elements.  The right way to define a unitary matrix is a bit subtle.  In the finite dimensional case, unitary matrices have to be square.  However, if $U=(|e₁〉\ ⋯\ |e_n〉\ U')$, where $U†U=I$, it seems right to call $U$ unitary.  In effect, we have written a few orthonormal columns out explicitly, and left the others implicit in $U'$.  So perhaps any infinite matrix where $U†U$ is an infinite identity matrix should be regarded as unitary.

We are setting the Hermitian conjugate of a ket to the corresponding bra, the Hilbert space analog of row vectors being conjugate to column vectors.  Dirac draws a distinction between conjugate complexes such as observables, which can be added to form real parts and subtracted to form imaginary ones, and complex imaginaries such as bras and kets, which can not.  The difference seems a bit mysterious in his book, but it is now clear.  Linear mappings from a space into itself have the same form as their Hermitian conjugates, linear mappings between two different spaces do not.  Bras and kets map Hilbert space into $\bf C$, so of course their types differ from those of their conjugates.

As well as mappings into ket space, we will consider linear mappings into the direct sum of ket space and ${\bf C}^n$.  Elements of the sum space are column vectors of one ket and $m$ complex numbers, of the form $(|b₀〉,b₁,…,b_m)^{\rm T}$.  These occur in Tychonoff regularisation, where some least-squares residuals are functions of $|ψ(z)〉$ and others of $z$ itself.  Linear mappings from ${\bf C}^n$ into these vectors are represented by an $m×n$ matrix whose first row is kets, and lower rows complex numbers: $$|A〉'=\pmatrix{|a_{01}〉& |a_{02}〉&& |a_{0n}〉\cr a_{11}&a_{12}&&a_{1n}\cr &&\ddots&\cr a_{m1}&a_{m2}&&a_{mn}}.$$  Following Matlab, we will denote such matrices by $(|A〉;A)$, where $|A〉$ is the first row, and $A$ the rest of the matrix.

We have been thinking of $|A〉$ as $1×r$ row vector, with elements $|a_i〉$.  We can also think of it as a $∞×r$ matrix, where the $|a_i〉$ are columns of infinite length.  We can then think about factoring it into QR or SVD form, which afford stable solutions to ill-conditioned least squares problems.

The singular value decomposition is $|A〉=|\hat U〉\hat Σ{\hat V}*$.  Here, $\hat Σ$ is a diagonal $r'× r'$ matrix of real singular values, where $r'$ is the rank of $|A〉$, the number of $|a_i〉$ that are linearly independent.  The complex matrix $\hat V$ is $r×r'$, and its columns ${\hat V}_i$ are orthonormal vectors in ${\bf C}^r$.  This is the same as the finite dimensional case.  The different bit is $|\hat U〉$, a row vector of $r'$ kets: the left singular vectors $|u₁〉$ through $|u_{r'}〉$ lie in Hilbert space.

A form $|A〉=|Q〉R$ could be computed by by Gramm-Schmidt.  Once this is done, the SVD of the complex matrix $R=UΣV*$ can be computed as usual, giving an SVD for $|A〉=(|Q〉U)ΣV*$.  That might allow the use of much more stable algorithms for solving least squares problems in Hilbert space.

A full $|Q〉R$ factorisation can be computed by Householder reflections.  It is unclear whether this differs from modified Gramm-Schmidt in the infinite case.  The idea is to multiply $|A〉$ on the left by unitary operators $Q_i†$.  The first step is $$ Q₁†|A〉=\pmatrix{〈q₁|\cr J₁} \pmatrix{|A₁〉&⋯&|A_n〉}=\pmatrix{r₁&r₂&…&r_n\cr |∅〉&|A₂'〉&…& |A_n'〉};$$ the second step is $$Q₂†Q₁†|A〉=\pmatrix{1&〈∅|\cr0&〈q₂|\cr |∅〉&J₂}\pmatrix{r₁&r₂&…&r_n\cr |∅〉&|A₂'〉&…& |A_n'〉}=\pmatrix{r₁&r₂&…&r_n\cr 0&r₂'&…&r_n'\cr|∅〉&|∅〉&…& |A_n''〉}.$$  Eventually, a zero ket has been introduced in every column of the product, and an upper triangular form obtained.  We are extracting an orthonormal basis one row at a time.  I haven't figured out the details of the reflections $(|q〉\ J)†$.  For finite matrices, Householder reflections are efficient because the columns you manipulate get shorter at each step.  Infinite columns stay infinite, so it isn't clear this will beat modified Gramm-Schmidt.  In fact, they might exactly the same algorithm in the infinite case: the first reflector subtracts the component of $|A₁〉$ from each column, and adds a row of complex coefficients.  That's exactly what Gramm-Schmidt does.

Because the Jacobian matrix $|Dψ(z)〉$ is numerically rank-deficient, least squares problems involving it are ill-posed.  According to Trefethen \& Bau, only the SVD is stable in these cases.  I'll need a way to compute that; I believe the standard ones are based on $QR$ factorisation.

The mapping $|Dψ〉$ is defined by the condition $$|ψ(z+h)〉-|ψ(z)〉=|Dψ(z)〉h+O(h²);$$ if there is no such  $|Dψ(z)〉$, the function $|ψ〉$ is not differentiable at $z$.  For the matrix products to be well formed, this must be a $1×r$ vector $|Dψ〉=(|∂₁ψ〉,|∂₂ψ〉,…,|∂_rψ〉)$, where $r$ is the number of variational parameters, the length of the vector $z$.


\beginsection{The states we compute with}

The goal is to do variational mechanics, in second quantisation, using multimode coherent states.  These might be projected onto a constant total particle number.  The Hamiltonians, in second quantisation, tend to be low-degree polynomials in the creation and annihilation operators of the modes; there will be very many modes for 3 dimensional systems, and thus the polynomials will have very many terms.

Consider states of the form $$|Π[P](φ,α)〉=P(α,α*,a†)\exp(φ+a†α)|0〉.$$  Here, $α$ is a column vector of amplitudes, $a$ a column vector of creation operators for the modes, and $P$ a polynomial in $3(m-1)$ variables.  Note that $a_i|Π[P](φ,α)〉=α_i|Π[P](φ,α)〉=|Π[π_iP](φ,α)〉$, and $a†_i|Π[P](φ,α)〉=|Π[π_{2m+i}P](φ,α)〉$, and $$|DΠ[P](φ,α)〉=[1,a†]|Π[P](φ,α)〉=\bigl(|Π[P](φ,α)〉,|Π[π_{2m+1}P](φ,α)〉,…,|Π[π_{3m}P](φ,α)〉\bigr).$$  Here $π_i$ is the monomial $π_i(z₁,…,z_n)=z_i$.  Accordingly, sums of such states are closed under every operation we want to perform on them.  Furthermore, their inner products can be calculated, and come out to $Q(α',α*)\exp(φ'+φ*+α*α')$, where $Q$ is a normally ordered polynomial.

The Python module {\tt states} is a preliminary implementation of this.  In practice, it's easier to factor the polynomial in the raising operator through the exponential, and represent $$D(α)∑_n c_n|n〉,$$ where $D(α)$ is a displacement operator and $|n〉$ are number states.  The library currently has two limitations.  It only represents states of one mode, and the $c_n$ are restricted to complex numbers, not polynomials in $α$ and $α*$.  Relaxing either restriction on its own would be straightforward, and worthwhile.  With multiple modes, we could investigate Hubbard models.  With polynomials, we could compute the Jacobian of the map $|ψ(z)〉→\dot z$, and get a handle on the stiffness of the variational equations.

To relax both at the same time would require massively multivariate polynomials.  Computing with multivariate polynomials is be an open area of research—see Stetter's textbook—but I have some ideas which should suffice for our purposes.

It also has a serious numerical instability, that is intrinsic to superpositions of non-orthogonal states.  Suppose we have two states $|α〉$ and $|α+h〉$, with $h«1$, and we wish to find $||α+h〉-|α〉|$.  A stable way is to expand the states over an orthonormal basis, so that $|α〉=∑c_n|n〉$, and $|α+h〉=∑d_n|n〉$.  The $c_n$ and $d_n$ are of order 1; their differences $d_n-c_n$ are of order $h$; summing these in quadrature is stable.  So this algorithm multiplies the relative error by $h^{-1}$: we can't expect to do much better that that.

The library, and the scripts that preceded it, work differently.  They calculate the quantities $〈α|α〉$, $〈α+h|α+h〉$, and $〈α|α+h〉$.  All these are of order 1.  They are combined to give $$||α+h〉-|α〉|²=〈α|α〉+〈α+h|α+h〉-2\Re 〈α|α+h〉,$$ a quantity of order $h²$.  So, in this case, our method amplifies the error by $h^{-2}$ instead of $h^{-1}$.  In other cases, say when we evaluate the quadratic difference $|α+h〉-|α〉-|Dα〉h$, things are even worse.  Some scripts amplify a floating point $ε$ of $10^{-16}$ to about $10^{-4}$!

Fortunately, there are ways to fix this instability.  Because we're representing displaced sums of Fock states, we can combine states with similar amplitudes, expand the first few terms in the Taylor series of the relative displacement operators, and apply them to the number states.  This is a disguised version of the standard algorithm for summing Gaussians, where you expand them as sums of orthogonal polynomials centred on cluster points.  Tim and Bogdan have used this, and apparently the algorithms for selecting cluster points are very well developed.  All that's needed is a {\tt stabilise} method on sum states, to do the clustering.  This would be called after numbers were substituted, immediately before the products were taken.

Finally, the library is quite inefficient, when run on interpreted Python.  There are a few ways to fix this.  The easy one would be to port it to a compiled language.  Haskell and Go spring to mind.  The drawback is that we'd lose the ease of writing scripts.  The Pyplot graphics library is a pain to learn, but really useful once you figure out how it works.  Also, this is a short-term answer: when I find a stable algorithm, we'll want to run it on parallel hardware.

The other road to efficiency is to write a library for block-pattern matrices.  When a matrix decomposes into blocks, and each block is computed in a similar way from parameters—in coherent DFM, the elements are polynomials in the parameters, and the whole block is multiplied by an exponentiated polynomial—then you can compute the pattern once, and hand it and the parameters to a kernel, that generates the matrix efficiently.  You can even reuse the same pattern at every timestep.  This would make the parallel port as simple as possible; we'd just have to port the kernel.  This approach would be more complicated if SVDs or clustered sum states were used; but doing those things in parallel is essentially complex.


\beginsection{How Dirac-Frenkel fails}

Given a state $|ψ〉$, the Dirac-Frenkel time derivative minimises the difference between the sides of Schrödinger's equation, $\|i\hbar{{\rm d}\over{\rm d}t}|ψ〉-H|ψ〉\|²$.  Finding this derivative is a linear least squares problem, because $${{\rm d}\over{\rm d}t}|ψ(\z(t))〉 = |{\rm D}ψ(\z(t))〉\dot\z,$$ where the Jacobian $|Dψ〉$ is a row vector of kets, and $\dot\z$ is a column vector of rates of change of parameters.  Below, we will consider nonlinear optimisiation algorithms, but these reduce the problem to a series of linear problems, so solving linear least squares problems will be a basic step in any algorithm we end up using.

Numerical analysis textbooks recommend solving such problems by reducing the Jacobian to an orthogonal form, either a QR factorisation, or a singular value decomposition.  As explained above, we could do that, given a library to manipulate the kets.  The form $N p(α,α*,a†) |α〉$, where $N$ is a projector onto constant total number, and $p$ a multivariate polynomial, is closed under addition and differentiation wrt $α$.  We can easily take inner product of kets of this form, and they represent all the kets we need.  So a library to represent these kets and linear combinations of them might be the way to go.

We are currently solving the problem by calculating coefficients of normal equations, and solving those numerically.  Trefethen and Bau give an example of a linear least squares problem where this method produces a 30\% error, but which other methods can solve to 5 or 10 decimal places.  They claim this happens for least squares problems where, in our context, the columns of the Jacobian are almost parallel, and the solution is a tight fit.  That's exactly what we have!

The usual way to solve ill-conditioned, linear least squares problems is Tychonoff conditioning.  Given a least squares problem $Ax≈y$, where slightly changing $y$ can change $x$ greatly, extra rows are appended to $A$ and $y$, which correspond to additional equations in $x$, in the hope that the new problem is better conditioned.  The magnitudes of these added rows are kept small enough that the new answer has a nearly minimal residual in the original problem.  Usually, the extra condition is $x≈0$.

Tychonoff conditioning gives a solvable problem at each timestep.  However, the resulting differential equation turns out to be unstable when discretised.  The continuous differential equation $\dot x=f(x(t))$ becomes a recurrence relation $$x_n+a_1x_{n-1}+⋯+a_Nx_{n-N} = δt\bigl(b₀f(x_n)+b₁f(x_{n-1})+⋯+b_Mf(x_{n-M})\bigr)$$ when discretised.  By taking a linear approximation to $f$, and diagonalising, this is approximated over short periods by a set of recurrences of the form $$x_n+a_1x_{n-1}+⋯+a_Nx_{n-N} = κδt\bigl(b₀x_n+b₁x_{n-1}+⋯+b_{n-M}\bigr),$$ and for certain values of $κδt$, the solutions to these diverge rapidly from the solution to the continuous equation.  This phenomenon is called stiffness.

When, as in our problem, the function $f$ is the solution to an ill-conditioned problem, stiffness is all but guaranteed.  The definition of "ill-conditioned" is that small changes in the input $x$ produce large changes in the computed derivative; this is, very nearly, also the definition of stiffness.  We have two ideas on how to get around this.  Both rely on the insight that the ill-conditioning and stiffness are accidental.  There are many ways to solve the least-squares problem, with nearly minimum residual; if the solution we choose gives unstable dynamics, we should choose another solution.

\beginsection{Nonlinear optimisation}

The first idea is to solve the dynamics over a finite timestep, instead of deriving a differential equation.  This would be done by finding a set of co-ordinates $z'$ to minimise the residual $\bigl| |ψ(z')〉-|ψ(z)〉+iH\,dt/\hbar|ψ(z)〉\bigr|².$  Because the chart from co-ordinates to kets is tightly curved, this optimisation would be done nonlinearly in the co-ordinates.

\centerline{\XeTeXpdffile "lmone-1.pdf"}

The basic way to do this, and the way it fails, are shown above.  The point $X$ represents $(1-iH\,δt/\hbar|ψ(z)〉$; the curved line is the manifold of possible $|ψ(z')〉$.  For simplicity, arc length along this curve is directly proportional to $z'$.  Point $A$ represents an initial guess for $|ψ(z')〉$.  The nonlinear co-ordinates are first approximated linearly, giving the tangent line $Ab$, and the linear least-squares problem along this line is solved, giving the point $b$.  This is not a possible $|ψ(z')〉$, so we choose the point $B$, which lies on the curve, at the same co-ordinates as $b$ has on the tangent line.  Repeating this step takes us to point $C$; it is clear that linear approximations will oscillate forever, and not converge to the actual least-squares solution, at the top of the curve.  The problem is that, at the linear solutions $b$ and $c$, the error in the linear approximation dominates the residual in the linearised problem.

\centerline{\XeTeXpdffile "lmone-2.pdf"}

This diagram shows the canonical way around this problem, Levenberg-Marquardt optimisation.  Instead of solving the linearised problem as it is, the solution is constrained to a part of the tangent line that accurately approximates the original curve, or at least is closer to the curve than to the goal point $X$.  As shown, this converges to the non-linear solution.

There are two issues with this.  The first is choosing the constraint lengths $Ab$ and $Bc$.  In practice, this seems be done by trial and error, shortening the allowed length until the computed solution lies close to the curve.

There's a more basic problem.  Given that line $AX$ represents $|ψ(z')〉-|ψ(z)〉+iH\,dt/\hbar|ψ(z)〉$, Levenberg-Marquart optimisation is very similar to solving the original differential equation with a reduced timestep.  (The only difference is that, if solving the ODE, point $X$ would be recomputed from the new co-ordinates at each step.)  It is not at all clear how this type of optimisation could solve fewer linear least squares problems than an adaptive ODE solver.  Conversely, if the problem is stiff enough that ODE solvers don't work, it is unclear how nonlinear optimisation could do any better.

But let's ignore this, and see how Levenberg-Marquardt would work.  The inputs are a parameter vector $z$, and a timestep $δt$.  The output is a step $δz$ that minimises $$\bigl\| |ψ(z+δz)〉-(1-iHδt/\hbar)|ψ(z)〉\bigr\|².$$  As always in iterative methods, a big issue is when to stop iterating and annoint the current approximation as the answer.  I think appropriate conditions for our problem would be that, if the final step is doubled, the linearisation error stays small compared to the residual, and the residual after the doubled step is larger than after the original step.  Possibly we should also require the double-step residual to be similar to the residual before the final step was taken.  The algorithm looks as follows (except that I haven't formalised the new stopping condition).

\item{1} Set $δz←0$, $r←0$.
\item{2} Initialise $ε$ somehow.
\item{3} Orthogonally project $\bigl(|ψ(z+δz)〉-(1-iHδt/\hbar)|ψ(z)〉\bigr); 0$ onto the column space of $|Dψ(z+δz)〉; εI$. Set $|ψ'〉$ to the result, and $w$ to the coefficients of the columns.
\item{5} Compare $\bigl\| |ψ(z+δz+w)〉-|ψ'〉\bigr\|$ to $\bigl\| (1-iHδt/\hbar)|ψ(z)〉-|ψ'〉\bigr\|$.  Go to 6 if big, 7 if small, 8 if intermediate.
\item{6} Set $ε←2ε$ and go to 3.
\item {7} Set $ε←ε/2$.
\item{8} Set $δz←δz+w$.
\item{9} Set $r'←\bigl\| (1-iHδt/\hbar)|ψ(z)〉-|ψ(z+δz)〉\bigr\|²$.
\item{10} If $r'$ is significantly less than $r$, set $r←r'$ and go to 3.
\item{11} Done.

Marquardt is supposed to have modified Step~3 somehow, using the diagonal of $〈Dψ(z+δz)|Dψ(z+δz)〉$ instead of $I$, but his paper is hard to get from Swinburne.

To do the projection naively, we will need the moment $〈Dψ(z+δz)|\bigl(|ψ(z+δz)〉-(1-iHδt/\hbar)|ψ(z)〉\bigr)$.  This is derived in the Appendix.  We will have to beware catastrophic cancellation when we compute it, of course.

\beginsection{Explicit softening}

Another approach would be to condition in a way that directly improved the stiffness of the ODEs.  Traditional Tychonoff regularisation should to this to some extent, because it improves the condition number of the matrix; see Wikipedia.  The one vector that should change gradually with $|ψ(z)〉$ is $H|ψ(z)〉$; if this changes sharply, the stiffness is not just a matter of co-ordinates, but fundamental to the dynamics.  To ensure the computed $\dot z$ changes as gradually as possible with $z$, we need to constrain the relations between components of $\dot z$ for which $|∂_jψ(z)〉{\dot z}_j$ point in similar directions.  An obvious way is to constrain their ratios to the relative components of $H|ψ(z)〉$, by minimising something proportional to $|{\dot z}_j〈ψ(z)|H|∂_kψ(z)〉-{\dot z}_k〈ψ(z)|H|∂_jψ(z)〉|²$. 

This still gives a linear problem; it shouldn't affect the solution too much, because we're constraining relations between co-ordinates that are nearly parallel.  The weights ought to be chosen high for parallel co-ordinates, and low for the others.  The simple option would be to choose weights propoirtional to $|〈∂_jψ(z)|∂_kψ(z)〉|²/〈∂_jψ(z)|∂_jψ(z)〉〈∂_kψ(z)|∂_kψ(z)〉$.  In principle, there are $N(N-1)$ such constraints for $N$ dimensions.  The ones with small weights can be rounded to zero --- the physics will constrain those directions.  Among the high weights, there is still some transitive redundancy: if we constrain the ratios $a:b$ and $b:c$, we are also constraining the ratio $a:c$.

\beginsection{Spectral or phasespacetime method}

The Dirac-Frenkel approach, as we use it, treats time differently than phase space.  At a single time, the quantum state is represented as a discrete set of phases, but these phases evolve continuously in time.  They seem to evolve continuously with great reluctance on their part, and great effort on our computer's.

We can discretise using coherent states because harmonic oscillators have a natural scale in phase space, set by the uncertainty principle and the symmetry of the Hamiltonian with respect to phase.  But Harmonic oscillators also have a natural scale in time: their frequency.  We could represent the state, over the simulation interval, as the sum of a discrete set of coherent states, whose amplitudes vary in time as Gaussians, with width $ω⁻¹$.  These would be adjusted variationally to minimise the integral of the residual of Schrödinger's equation, with some constraint on the initial state.  We could sample the initial state, fix those amplitudes, and vary the rest.

Squeezed states might be useful, too.  You can think about reducing the amplitude, and increasing the duration, so that the area under the Gaussian stays the same.


\beginsection{Implementation}

The library in the module {\tt states.py} is based on a class {\tt State}, representing a quantum state regardless of it being a bra or a ket.  In practice, {\tt State} implements a ket: its methods are called directly by the class {\tt Ket}, and with conjugated arguments by the class {\tt Bra}.  Subclasses of {\tt State} represent classes of kets in specific ways.  The subclass {\tt FockExpansion} expands a state over number states, {\tt Sum} is exactly what it sounds like, and {DisplacedState} represents a state of the form $D(α)|ψ〉$, where $|ψ〉$ is represented by another subclass.

{\tt Bra}, {\tt Ket} and {\tt Operator} implement Dirac algebra.  Scalar and inner products are associative.  Outer products are not yet implemented, but would be a straightforward extension.  The notation above is implemented by the class {Matrix}.  It should be possible to do it with {\tt ndarray} of {\tt Bra} or {\tt Ket}, but the Numpy code for general objects is so buggy that it's easier to write one's own.

Addition of {\tt State} is entirely straightforward: it forms a superposition, by returning the {\tt Sum} of the operands if need be.  When two {\tt State}s multiply, the left operand is treated as a bra, and the right one as a ket.  This has some tricky consequences.  Let $z$ be a scalar, {\tt A} a {\tt State} representing $|a〉$, and {\tt B} one representing $|b〉$.  Then {\tt z*(A*B)} and {\tt A*(z*b)} both evaluate to the Dirac product $z〈a|b〉$, but the very tempting {\tt (z*A)*B} evaluates to $z*〈a|b〉$, because {\tt z*A} represents the bra $〈a|z*$.  Therefore, products between states and scalars are not directly supported: one must say {\tt A.scaled(z)}, a reminder that one might mean {\tt A.scaled(conjugate(z))}.

Products of {\tt DisplacedState} are computed according to $$\eqalign{D†(β)D(α)&=D(-β)D(α)=e^{-βa†+β*a}e^{αa†-α*a} \cr &=e^{[-βa†+β*a,αa†-α*a]/2}e^{(α-β)a†-(α-β)*a} \cr &=e^{(β*α-βα*)/2}D(α-β)=e^{(β*α-βα*-(α*-β*)(α-β))/2}e^{(α-β)a†}e^{-(α-β)*a}\cr &=e^{-½(|α|²+|β|²)+β*α}e^{(α-β)a†}e^{-(α-β)*a}.}$$  Products of sums are computed by adding products of terms: this has consequences for stability, that were discussed above.

Coherent states, and sums of them, have parameters that are logarithmic weights and coherent amplitudes.  The parameters are returned by {\tt q.prms()}, and a modified state is constructed by, say, {\tt q.smrp(q.prms() + h)}.  The method {\tt State.prms} returns a tuple, {\tt Ket.prms} returns a column, and {\tt Bra.prms} a row.  Wirtinger calculus is implemented by the methods {\tt D} and {\tt C}, such that {\tt q.smrp(q.prms() + h)} is approximated by {\tt q + q.D()*h + h.conjugate()*q.C()} for small {\tt h}.  These can be multiplied on the right by small changes in parameters.  Usually, {\tt C()} is zero for a ket, and {\tt D()} is zero for a bra.

\bye